checkpoints_path = "/home/lucas/Documents/checkpoints/"
# checkpoints_path = "/gpfsdswork/projects/rech/fbe/uaa31dq/checkpoints/"
# checkpoints_path = "/home/lucas/geovic_mount/checkpoints/"

transformer_configs = {
    "Llama-2-7B-chat-hf": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Llama-2-7B-chat-hf",
        n_layers=32,
        n_heads=32,
        dim=4096,
        rope_base=10000,
    ),
    "Llama-2-13B-chat-hf": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Llama-2-13b-chat-hf",
        n_layers=40,
        n_heads=40,
        dim=5120,
        rope_base=10000,
    ),
    "Mistral-7B": dict(
        checkpoint_path=checkpoints_path + "mistralai/Mistral-7B-Instruct-v0.2",
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=32000,
        rope_base=10000,
    ),
    "Meta-Llama-3-8B": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Meta-Llama-3-8B",
        block_size=8192,
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=128256,
        rope_base=10000,
    ),
    "Meta-Llama-3-8B-Instruct": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Meta-Llama-3-8B-Instruct",
        block_size=8192,
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=128256,
        rope_base=10000,
    ),
    "Meta-Llama-3.1-8B": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Meta-Llama-3.1-8B",
        block_size=8192,
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=128256,
        rope_base=10000,
    ),
    "Meta-Llama-3.1-8B-Instruct": dict(
        checkpoint_path=checkpoints_path + "meta-llama/Meta-Llama-3.1-8B-Instruct",
        block_size=8192,
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=128256,
        rope_base=10000,
    ),
    "llama3-llava-next-8b-hf": dict(
        checkpoint_path=checkpoints_path + "llava-hf/llama3-llava-next-8b-hf",
        vocab_size=128320,
        intermediate_size=14336,
        block_size=8192,
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        rope_type="hf",
        rope_base=10000,
    ),
    "TinyLlama-1.1B-Chat-v1.0": dict(
        checkpoint_path=checkpoints_path + "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        n_layers=22,
        n_heads=32,
        dim=2048,
        intermediate_size=5632,
        vocab_size=32000,
        rope_base=10000,
    ),
    "TinyLlama-1.1B-intermediate-step-1431k-3T": dict(
        checkpoint_path=checkpoints_path
        + "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T",
        block_size=2048,
        vocab_size=32000,
        intermediate_size=5632,
        n_layers=22,
        n_heads=32,
        n_local_heads=4,
        dim=2048,
        rope_base=10000,
    ),
    "Mistral-7B-Instruct-v0.2": dict(
        checkpoint_path=checkpoints_path + "mistralai/Mistral-7B-Instruct-v0.2",
        n_layers=32,
        n_heads=32,
        n_local_heads=8,
        dim=4096,
        intermediate_size=14336,
        vocab_size=32000,
        rope_base=10000,
    ),
    "gemma-7b-it": dict(
        checkpoint_path=checkpoints_path + "google/gemma-7b-it",
        dim=3072,
        vocab_size=256000,
        n_layers=28,
        n_heads=16,
        n_local_heads=16,
        intermediate_size=24576,
        head_dim=256,
        rope_base=10000,
        activation="gelu",
    ),
    "gemma-2b-it": dict(
        checkpoint_path=checkpoints_path + "google/gemma-2b-it",
        dim=2048,
        block_size=8192,
        vocab_size=256000,
        n_layers=18,
        n_heads=8,
        n_local_heads=1,
        intermediate_size=16384,
        rope_base=10000,
        activation="gelu",
    ),
    "paligemma-3b-mix-224": dict(
        checkpoint_path=checkpoints_path + "google/paligemma-3b-mix-224",
        block_size=8192,
        dim=2048,
        vocab_size=257216,
        n_layers=18,
        n_heads=8,
        n_local_heads=1,
        intermediate_size=16384,
        rope_base=10000,
        activation="gelu",
    ),
    "paligemma-3b-pt-896": dict(
        checkpoint_path=checkpoints_path + "google/paligemma-3b-pt-896",
        block_size=8192,
        dim=2048,
        vocab_size=257216,
        n_layers=18,
        n_heads=8,
        n_local_heads=1,
        intermediate_size=16384,
        rope_base=10000,
        activation="gelu",
    ),
    ## for test purporse
    "test": dict(
        checkpoint_path=checkpoints_path + "test/test",
        n_layers=3,
        n_heads=2,
        dim=64,
        biais=True,
    ),
}
